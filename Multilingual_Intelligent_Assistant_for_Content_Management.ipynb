{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# 1. Initialize FastAPI app\n",
        "app = FastAPI(title=\"Multilingual Intelligent Assistant\")\n",
        "\n"
      ],
      "metadata": {
        "id": "c3OImvPDW9TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Optimized Model Loading (Global scope to load once)\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f\"Loading models on {'GPU' if device == 0 else 'CPU'}...\")\n",
        "\n",
        "# Loading smaller, optimized versions of models for faster inference\n",
        "gen_pipe = pipeline(\"text-generation\", model=\"gpt2\", device=device)\n",
        "sum_pipe = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-6-6\", device=device)\n",
        "trans_pipe = pipeline(\"translation\", model=\"NAMAA-Space/masrawy-english-to-egyptian-arabic-translator-v2.9\", device=device)\n",
        "qa_pipe = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\", device=device)\n"
      ],
      "metadata": {
        "id": "R3_nofvKaD93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Define Request Schema\n",
        "class AIRequest(BaseModel):\n",
        "    topic: str\n",
        "    question: str\n",
        "\n",
        "# 4. Define Response Schema\n",
        "class QAResponse(BaseModel):\n",
        "    question: str\n",
        "    answer: str\n",
        "\n",
        "class AIResponse(BaseModel):\n",
        "    topic: str\n",
        "    generated_text: str\n",
        "    summary: str\n",
        "    translated_summary_ar: str\n",
        "    question_answer: QAResponse\n"
      ],
      "metadata": {
        "id": "tML-VKB-bd05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. API Endpoint\n",
        "@app.post(\"/ai-assistant\", response_model=AIResponse)\n",
        "def run_assistant(request: AIRequest):\n",
        "    try:\n",
        "        # Step A: Generate Content\n",
        "        # We use max_new_tokens for precise control over output length\n",
        "        prompt = f\"Explain the impact of {request.topic}:\"\n",
        "        generated = gen_pipe(prompt, max_new_tokens=100, do_sample=True)[0]['generated_text']\n",
        "\n",
        "        # Step B: Summarize\n",
        "        summary = sum_pipe(generated, max_length=40, min_length=10, do_sample=False)[0]['summary_text']\n",
        "\n",
        "        # Step C: Translate Summary to French\n",
        "        translated = trans_pipe(summary)[0]['translation_text']\n",
        "\n",
        "        # Step D: Answer Question based on Generated Content\n",
        "        qa_result = qa_pipe(question=request.question, context=generated)\n",
        "\n",
        "        # Return structured JSON response\n",
        "        return {\n",
        "            \"topic\": request.topic,\n",
        "            \"generated_text\": generated,\n",
        "            \"summary\": summary,\n",
        "            \"translated_summary_ar\": translated,\n",
        "            \"question_answer\": {\n",
        "                \"question\": request.question,\n",
        "                \"answer\": qa_result['answer']\n",
        "            }\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n"
      ],
      "metadata": {
        "id": "lJCwM-xMbfPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install nest_asyncio"
      ],
      "metadata": {
        "id": "w759QRTYclM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "from fastapi import FastAPI\n",
        "\n",
        "# 1. Allow nested loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# 2. Define a function to run the server\n",
        "def run_server():\n",
        "    # 'app' must be the FastAPI instance you defined earlier\n",
        "    config = uvicorn.Config(app, host=\"127.0.0.1\", port=8000, log_level=\"info\")\n",
        "    server = uvicorn.Server(config)\n",
        "    server.run()\n",
        "\n",
        "# 3. Start the server in a separate thread\n",
        "# This prevents the \"RuntimeError\" by not blocking the main notebook loop\n",
        "server_thread = threading.Thread(target=run_server)\n",
        "server_thread.start()\n",
        "\n",
        "print(\"ðŸš€ Server is running in the background at http://127.0.0.1:8000\")"
      ],
      "metadata": {
        "id": "EKYFO5H5bjSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "payload = {\n",
        "    \"topic\": \"Vitamins Importance\",\n",
        "    \"question\": \"How do Vitamins help health of people?\"\n",
        "}\n",
        "\n",
        "response = requests.post(\"http://127.0.0.1:8000/ai-assistant\", json=payload)\n",
        "print(response.json()['topic'])\n",
        "print(response.json()['generated_text'])\n",
        "print(response.json()['summary'])\n",
        "print(response.json()['translated_summary_ar'])"
      ],
      "metadata": {
        "id": "qU_ccyZxdbJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rl7Krh70dxe5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}